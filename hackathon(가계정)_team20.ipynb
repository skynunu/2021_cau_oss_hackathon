{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "hackathon.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/skynunu/2021_cau_oss_hackathon/blob/main/hackathon2_team20.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Q9b24tHeWdS"
      },
      "source": [
        "# tensorflow와 tf.keras 및 관련 라이브러리 임포트\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow import keras\n",
        "from keras.utils import np_utils\n",
        "import numpy as np\n",
        "\n",
        "# 데이터셋 로드 (Training dataset: CIFAR10, test dataset: CIFAR10 & CIFAR10의 변형)\n",
        "(x_train, y_train), (x_test1, y_test1) = keras.datasets.cifar10.load_data()\n",
        "\n",
        "test_ds2 = tfds.load('cifar10_1/v6', split='test', shuffle_files=False, batch_size=-1)\n",
        "test_ds2 = tfds.as_numpy(test_ds2)\n",
        "x_test2, y_test2 = test_ds2['image'], test_ds2['label']\n",
        "\n",
        "#분류를 위해 클래스 벡터를 바이너리 매트릭스로 변환\n",
        "y_train = np_utils.to_categorical(y_train)\n",
        "y_test1 = np_utils.to_categorical(y_test1)\n",
        "y_test2 = np_utils.to_categorical(y_test2)\n",
        "\n",
        "# 총 클래스 개수\n",
        "num_classes = y_train.shape[1]\n",
        "input_shape = x_train.shape[1:]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lS-NtkULegl9"
      },
      "source": [
        "# 데이터 전처리 (예: normalization)\n",
        "# 원본 데이터와 전처리 후 데이터를 구분하기 위해, 변수명 x_train_after, x_test1_after, x_test2_after를 변경하지 말 것\n",
        "x_train_after = x_train / 255.0\n",
        "x_test1_after = x_test1 / 255.0\n",
        "x_test2_after = x_test2 / 255.0\n",
        "\n",
        "x_train_after= x_train_after.astype('float32')\n",
        "x_test1_after= x_test1_after.astype('float32')\n",
        "x_test2_after= x_test2_after.astype('float32')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2W04dTTXekuU"
      },
      "source": [
        "# train dataset data augmentation\n",
        "batch_size =64\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True )\n",
        "\n",
        "train_dataset = datagen.flow(x_train_after, y_train, batch_size=batch_size)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEKV2T0aemiE"
      },
      "source": [
        "# batch 사이즈 설정 test1_after, test2_after\n",
        "BATCH_SIZE =64\n",
        "\n",
        "test1_dataset = tf.data.Dataset.from_tensor_slices((x_test1_after, y_test1))\n",
        "test2_dataset = tf.data.Dataset.from_tensor_slices((x_test2_after, y_test2))\n",
        "\n",
        "test1_dataset = test1_dataset.batch(BATCH_SIZE)\n",
        "test2_dataset = test2_dataset.batch(BATCH_SIZE)\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIPbMFiTeotE"
      },
      "source": [
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout, GlobalAveragePooling2D,Activation, BatchNormalization\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.applications import VGG16, VGG19\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mw852QqpeqxW"
      },
      "source": [
        "     model = Sequential([ \n",
        "        Conv2D(64, (3, 3), kernel_initializer='he_uniform', padding='same', input_shape=(32, 32, 3)),\n",
        "        BatchNormalization(),\n",
        "        Activation('relu'),\n",
        "        Conv2D(64, (3, 3), kernel_initializer='he_uniform', padding='same'),\n",
        "        BatchNormalization(),\n",
        "        Activation('relu'),                 \n",
        "        MaxPooling2D((2, 2)),\n",
        "\n",
        "        Conv2D(128, (3, 3), kernel_initializer='he_uniform', padding='same'),\n",
        "        BatchNormalization(),\n",
        "        Activation('relu'),\n",
        "        Conv2D(128, (3, 3), kernel_initializer='he_uniform', padding='same'),\n",
        "        BatchNormalization(),\n",
        "        Activation('relu'),\n",
        "        MaxPooling2D((2, 2)),\n",
        "\n",
        "        Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'),\n",
        "        BatchNormalization(),\n",
        "        Activation('relu'),\n",
        "        Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'),\n",
        "        BatchNormalization(),\n",
        "        Activation('relu'),\n",
        "        Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'),\n",
        "        BatchNormalization(),\n",
        "        Activation('relu'),\n",
        "        MaxPooling2D((2, 2)),\n",
        "\n",
        "        Conv2D(512, (3, 3), kernel_initializer='he_uniform', padding='same'),\n",
        "        BatchNormalization(),\n",
        "        Activation('relu'),\n",
        "        Conv2D(512, (3, 3), kernel_initializer='he_uniform', padding='same'),\n",
        "        BatchNormalization(),\n",
        "        Activation('relu'),\n",
        "        Conv2D(512, (3, 3), kernel_initializer='he_uniform', padding='same'),\n",
        "        BatchNormalization(),\n",
        "        Activation('relu'),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        \n",
        "        #Flatten(),\n",
        "        GlobalAveragePooling2D(),\n",
        "        Dropout(0.5),\n",
        "         \n",
        "        Dense(512),\n",
        "        BatchNormalization(),\n",
        "        Activation('relu'),\n",
        "        Dropout(0.5),\n",
        "        \n",
        "        Dense(256),\n",
        "        BatchNormalization(),\n",
        "        Activation('relu'),\n",
        "        Dropout(0.5),    \n",
        "\n",
        "        Dense(10, activation='softmax'),\n",
        "\n",
        "        \n",
        "     ])\n",
        " "
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGr7apgfe0Vl"
      },
      "source": [
        "# model compile\n",
        "from keras import optimizers\n",
        "#'adam'\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ER2v6LW-e1dE"
      },
      "source": [
        "# model 체크포인트 생성\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath='/content/checkpoint_entire_best.h5', monitor='val_accuracy', verbose=1, save_weight_only=False, save_best_only=True, mode='auto')"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1P9dEfGae2-0"
      },
      "source": [
        "# model early point 설정\n",
        "from keras.callbacks import EarlyStopping\n",
        "earlystopping = EarlyStopping\n",
        "earlystopping = EarlyStopping(monitor='val_loss',  \n",
        "                              patience=12,         \n",
        "                             )\n",
        "# 모니터 기준 설정 (val loss) \n",
        "# 12회 Epoch동안 개선되지 않는다면 종료 (patience)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GSPGOSSTe44D",
        "outputId": "2e2605df-b3d7-440d-b1f9-c1ac10f364b8"
      },
      "source": [
        "#model fit\n",
        "model.fit(train_dataset, epochs = 200,  callbacks=[cp_callback, earlystopping], validation_data=(test2_dataset))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "782/782 [==============================] - 58s 74ms/step - loss: 1.7363 - accuracy: 0.3739 - val_loss: 1.8186 - val_accuracy: 0.3950\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.39500, saving model to /content/checkpoint_entire_best.h5\n",
            "Epoch 2/200\n",
            "782/782 [==============================] - 57s 72ms/step - loss: 1.1533 - accuracy: 0.5988 - val_loss: 1.5041 - val_accuracy: 0.4945\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.39500 to 0.49450, saving model to /content/checkpoint_entire_best.h5\n",
            "Epoch 3/200\n",
            "782/782 [==============================] - 57s 73ms/step - loss: 0.9258 - accuracy: 0.6921 - val_loss: 1.5508 - val_accuracy: 0.5300\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.49450 to 0.53000, saving model to /content/checkpoint_entire_best.h5\n",
            "Epoch 4/200\n",
            "782/782 [==============================] - 57s 73ms/step - loss: 0.8042 - accuracy: 0.7359 - val_loss: 1.5026 - val_accuracy: 0.5310\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.53000 to 0.53100, saving model to /content/checkpoint_entire_best.h5\n",
            "Epoch 5/200\n",
            "782/782 [==============================] - 57s 72ms/step - loss: 0.7170 - accuracy: 0.7667 - val_loss: 0.9960 - val_accuracy: 0.6580\n",
            "\n",
            "Epoch 00005: val_accuracy improved from 0.53100 to 0.65800, saving model to /content/checkpoint_entire_best.h5\n",
            "Epoch 6/200\n",
            "697/782 [=========================>....] - ETA: 6s - loss: 0.6508 - accuracy: 0.7885"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YA7HXJqUe6mM"
      },
      "source": [
        "save_path = '/content/'\n",
        "team_name = 'team20_transferlearning'\n",
        "\n",
        "# 트레이닝된 전체 모델을 저장합니다.\n",
        "model.save(save_path +  'model_entire_'+ team_name + '.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "en4cITfse9O0"
      },
      "source": [
        "### 모델 평가\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxYK85Qoe79s"
      },
      "source": [
        "save_path = '/content/'\n",
        "team_name = 'team20_transferlearning'\n",
        "\n",
        "model = keras.models.load_model(save_path + 'model_entire_' + team_name + '.h5')\n",
        "\n",
        "model.evaluate(x_test1_after, y_test1)\n",
        "model.evaluate(x_test2_after, y_test2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LS_QDJkrI6e"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
